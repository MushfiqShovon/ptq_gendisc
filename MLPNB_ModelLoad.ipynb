{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "537cb57a-da36-4def-88aa-d475ea420c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from models import MLPFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14e130dc-bedf-40f8-a5b5-09123e238ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import site\n",
    "import os\n",
    "os.environ['SP_DIR'] = '/opt/conda/lib/python3.11/site-packages'\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "# nlp library of Pytorch\n",
    "from torchtext import data\n",
    "\n",
    "import warnings as wrn\n",
    "wrn.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "SEED = 2021\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cuda.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a60dd76-c9d2-4145-b161-0f9764dcfdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = pd.read_csv('data/dbpedia/train.csv', header=None, usecols=[0,2])\n",
    "# train_data.columns = ['label', 'text']\n",
    "# valid_data = pd.read_csv('data/dbpedia/valid.csv', header=None, usecols=[0,2])\n",
    "# valid_data.columns = ['label', 'text']\n",
    "# test_data = pd.read_csv('data/dbpedia/test.csv', header=None, usecols=[0,2])\n",
    "# test_data.columns = ['label', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf22994a-ffb4-4fa7-b1dd-423e7a4a966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_text(text):\n",
    "#     return re.sub(r'[^A-Za-z0-9]+', ' ', str(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54db6043-407b-4c8a-8529-f015479dcc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data['text'] = train_data['text'].apply(clean_text)\n",
    "# valid_data['text'] = valid_data['text'].apply(clean_text)\n",
    "# test_data['text'] = test_data['text'].apply(clean_text)\n",
    "\n",
    "# train_data.to_csv('data/dbpedia/train_clean.csv', index=False, header=False)\n",
    "# valid_data.to_csv('data/dbpedia/valid_clean.csv', index=False, header=False)\n",
    "# test_data.to_csv('data/dbpedia/test_clean.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5307de6-e67d-408c-84e5-b0c45a65e49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_train_file = 'data/ag_news/train_clean.csv'\n",
    "cleaned_valid_file = 'data/ag_news/valid_clean.csv'\n",
    "cleaned_test_file = 'data/ag_news/test_clean.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed0c608f-4ac9-4f73-be33-e41bf4ff0377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': '4', 'text': ['Apple', 'yesterday', 'released', 'a', '12', '7', 'MB', 'security', 'update', 'that', 'consists', 'of', 'several', 'revised', 'components', 'including', 'Apache', 'AppKit', 'HIToolbox', 'Kerberos', 'Postfix', 'PSNormalizer', 'Safari', 'and', 'Terminal']}\n",
      "Number of instances per class: {'2': 30000, '3': 30000, '4': 30000, '1': 29999}\n",
      "Size of text vocab: 27723\n",
      "Size of label vocab: 4\n",
      "Using device: cuda\n",
      "Batch size initialized\n",
      "Number of instances per class: {'2': 30000, '3': 30000, '4': 30000, '1': 29999}\n"
     ]
    }
   ],
   "source": [
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "MAX_LEN=80\n",
    "\n",
    "def spacy_tokenizer(text):\n",
    "    tokens=[tok.text for tok in spacy_en.tokenizer(text)]\n",
    "    return tokens[:MAX_LEN]\n",
    "\n",
    "LABEL = data.LabelField()\n",
    "TEXT = data.Field(tokenize=spacy_tokenizer, batch_first=True, include_lengths=True)\n",
    "fields = [(\"label\", LABEL), (\"text\", TEXT)]\n",
    "\n",
    "training_data = data.TabularDataset(path=cleaned_train_file, format=\"csv\", fields=fields, skip_header=True)\n",
    "validation_data = data.TabularDataset(path=cleaned_valid_file, format=\"csv\", fields=fields, skip_header=True)\n",
    "test_data = data.TabularDataset(path=cleaned_test_file, format=\"csv\", fields=fields, skip_header=True)\n",
    "\n",
    "print(vars(training_data.examples[0]))\n",
    "\n",
    "train_data,valid_data = training_data, validation_data\n",
    "\n",
    "TEXT.build_vocab(train_data, \n",
    "                 max_size=40000, \n",
    "                 min_freq=5)\n",
    "\n",
    "LABEL.build_vocab(train_data)\n",
    "# Count the number of instances per class\n",
    "label_counts = {LABEL.vocab.itos[i]: LABEL.vocab.freqs[LABEL.vocab.itos[i]] for i in range(len(LABEL.vocab))}\n",
    "print(\"Number of instances per class:\", label_counts)\n",
    "\n",
    "\n",
    "print(\"Size of text vocab:\",len(TEXT.vocab))\n",
    "\n",
    "print(\"Size of label vocab:\",len(LABEL.vocab))\n",
    "\n",
    "TEXT.vocab.freqs.most_common(10)\n",
    "\n",
    "# Creating GPU variable\n",
    "device = torch.device(\"cuda\")\n",
    "#device = torch.device('cuda')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "\n",
    "BATCH_SIZE=32\n",
    "print(\"Batch size initialized\")\n",
    "# Count the number of instances per class\n",
    "label_counts = {LABEL.vocab.itos[i]: LABEL.vocab.freqs[LABEL.vocab.itos[i]] for i in range(len(LABEL.vocab))}\n",
    "print(\"Number of instances per class:\", label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff1baaea-94bb-49a5-9c83-4e2edadcba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator,validation_iterator = data.BucketIterator.splits(\n",
    "    (train_data,valid_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    # Sort key is how to sort the samples\n",
    "    sort_key = lambda x:len(x.text),\n",
    "    sort_within_batch = True,\n",
    "    device = device\n",
    ")\n",
    "\n",
    "test_iterator = data.BucketIterator(\n",
    "    test_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff4789a2-7a5f-47eb-a2ed-dbf45c1556d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10):  # Print 5 samples\n",
    "#     text, label = train_dataset[i]  # Index directly\n",
    "#     print(f\"Sample {i + 1}:\")\n",
    "#     print(f\"Text: {text}\")\n",
    "#     print(f\"Label: {label}\")\n",
    "#     print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d9f6164-a93e-4e25-9d82-f38674edc15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_class_accuracy(preds, y):\n",
    "    _, predicted = torch.max(preds, 1)\n",
    "    correct = (predicted == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def train(model,iterator,optimizer,criterion):\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        # cleaning the cache of optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text,text_lengths = batch.text\n",
    "        #print(\"Text Length:\", text_lengths[0].item())\n",
    "        global BATCH_SIZE\n",
    "        #BATCH_SIZE=text_lengths[0].item()\n",
    "        #print(\"Sent Length:\", BATCH_SIZE)\n",
    "        #print(\"Iterator Batch Size:\", batch.batch_size)\n",
    "        batch.batch_size=BATCH_SIZE\n",
    "        #print(\"Iterator Batch Size:\", batch.batch_size)\n",
    "        iterator = data.BucketIterator(\n",
    "            train_data,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            sort_key=lambda x: len(x.text),\n",
    "            sort_within_batch=True,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # forward propagation and squeezing\n",
    "        predictions = model(text).squeeze()\n",
    "        \n",
    "        # computing loss / backward propagation\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        #loss = criterion(predictions,batch.type)\n",
    "        loss.backward()\n",
    "        \n",
    "        # accuracy\n",
    "        acc = multi_class_accuracy(predictions,batch.label)\n",
    "        \n",
    "        # updating params\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    # It'll return the means of loss and accuracy\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model,iterator,criterion):\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    \n",
    "    # deactivate the dropouts\n",
    "    model.eval()\n",
    "    \n",
    "    # Sets require_grad flat False\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(iterator, desc=\"Evaluating\"):\n",
    "            text,text_lengths = batch.text\n",
    "            \n",
    "            predictions = model(text).squeeze()\n",
    "\n",
    "            #print(predictions.shape)\n",
    "              \n",
    "            #compute loss and accuracy\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = multi_class_accuracy(predictions, batch.label)\n",
    "            \n",
    "            #keep track of loss and accuracy\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef5ac8be-099f-473c-b293-b714bbe4157b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE_OF_VOCAB = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "NUM_HIDDEN_NODES = 100\n",
    "NUM_OUTPUT_NODES = len(LABEL.vocab)\n",
    "NUM_LAYERS = 1\n",
    "BIDIRECTION = False\n",
    "DROPOUT = 0.2\n",
    "BIT_WIDTH = 4  # This is unused in the current model but can be integrated later\n",
    "#class_priors = [0.25, 0.25, 0.25, 0.25]\n",
    "# Initialize model\n",
    "model = MLPFeatureExtractor(SIZE_OF_VOCAB, NUM_HIDDEN_NODES, NUM_OUTPUT_NODES)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6a7f1d6-86b1-4679-a9c8-5531d39d56e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "216ee9e5-0b4e-489d-8d9e-ff251c5a85a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the best model from MLPFeatureExtractor_ag_news_spacy.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dynamically generate the filename\n",
    "best_model_path = f\"MLPFeatureExtractor_ag_news_spacy.pth\"\n",
    "\n",
    "# # Initialize variables to track the best loss\n",
    "# best_valid_loss = float('inf')\n",
    "\n",
    "# num_epochs=100\n",
    "# # Training Loop\n",
    "# for epoch in range(1,num_epochs+1):\n",
    "    \n",
    "#     print(\"======================================================\")\n",
    "#     print(\"Epoch: %d\" %epoch)\n",
    "#     print(\"======================================================\")\n",
    "    \n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     train_loss,train_acc = train(model,train_iterator,optimizer,criterion)\n",
    "    \n",
    "#     valid_loss,valid_acc = evaluate(model,validation_iterator,criterion)\n",
    "    \n",
    "#     end_time = time.time()\n",
    "#     epoch_duration = end_time - start_time\n",
    "#     # Showing statistics\n",
    "#     print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "#     print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "#     print(f'\\tTime taken for epoch: {epoch_duration:.2f} seconds\\n')\n",
    "#     print()\n",
    "\n",
    "\n",
    "# torch.save(model.state_dict(), best_model_path)\n",
    "# print(f\"New best model saved at epoch {epoch+1} with validation loss {valid_loss:.4f}\")\n",
    "#Load the best model after training\n",
    "print(f\"Loading the best model from {best_model_path}\")\n",
    "model.load_state_dict(torch.load(best_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43ba1c57-a000-4b3e-baec-ee2a0573fb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loss, test_acc = evaluate(model,test_iterator,criterion)\n",
    "# print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62c80b1f-b7fe-4845-8eba-7c27d16a57d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(model, iterator, device):\n",
    "    model.eval()\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text,text_lengths = batch.text\n",
    "            labels = batch.label\n",
    "            features = model(text)\n",
    "            all_features.append(features.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "    \n",
    "    all_features = torch.cat(all_features, dim=0).numpy()\n",
    "    all_labels = torch.cat(all_labels, dim=0).numpy()\n",
    "    return all_features, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05ceb704-6755-44f5-82bf-0987fd756bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = extract_features(model, train_iterator, device)\n",
    "valid_features, valid_labels = extract_features(model, validation_iterator, device)\n",
    "test_features, test_labels = extract_features(model, test_iterator, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2324cb76-d17d-40b8-9b17-b7d782b74ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Normalize features\n",
    "scaler = MinMaxScaler()\n",
    "train_features = scaler.fit_transform(train_features)\n",
    "valid_features = scaler.transform(valid_features)\n",
    "test_features = scaler.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "893bd36c-dd20-4b35-a7f9-bdb8bcc30c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy with Naive Bayes: 0.9816\n",
      "Test Accuracy with Naive Bayes: 0.8680\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(train_features, train_labels)\n",
    "\n",
    "valid_preds = nb_model.predict(valid_features)\n",
    "valid_accuracy = accuracy_score(valid_labels, valid_preds)\n",
    "print(f\"Validation Accuracy with Naive Bayes: {valid_accuracy:.4f}\")\n",
    "\n",
    "test_preds = nb_model.predict(test_features)\n",
    "test_accuracy = accuracy_score(test_labels, test_preds)\n",
    "print(f\"Test Accuracy with Naive Bayes: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68956ae6-ad43-4158-b738-7157a7303e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brevitas.graph.quantize import preprocess_for_quantize\n",
    "from ptq_common import quantize_model, apply_bias_correction, apply_act_equalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c198b18-7eb1-475b-9316-b7fab30139d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_model = preprocess_for_quantize(\n",
    "            model,\n",
    "            equalize_iters=20,\n",
    "            equalize_merge_bias=True,\n",
    "            merge_bn=True,\n",
    "            channel_splitting_ratio=0.0,\n",
    "            channel_splitting_split_input=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d7aaa26-61e9-4100-909d-780e6b56b0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "dtype = getattr(torch, 'float')\n",
    "print(device)\n",
    "quant_model = quantize_model(\n",
    "        pre_model.to(device),\n",
    "        dtype=dtype,\n",
    "        device=device,\n",
    "        backend='layerwise',\n",
    "        scale_factor_type='float_scale',\n",
    "        bias_bit_width=32,\n",
    "        weight_bit_width=BIT_WIDTH,\n",
    "        weight_narrow_range=False,\n",
    "        weight_param_method='stats',\n",
    "        weight_quant_granularity='per_tensor',\n",
    "        weight_quant_type='sym',\n",
    "        layerwise_first_last_bit_width=BIT_WIDTH,\n",
    "        act_bit_width=BIT_WIDTH,\n",
    "        act_param_method='stats',\n",
    "        act_quant_percentile=99.9,\n",
    "        act_quant_type='sym',\n",
    "        quant_format='int',\n",
    "        layerwise_first_last_mantissa_bit_width=4,\n",
    "        layerwise_first_last_exponent_bit_width=3,\n",
    "        weight_mantissa_bit_width=4,\n",
    "        weight_exponent_bit_width=3,\n",
    "        act_mantissa_bit_width=4,\n",
    "        act_exponent_bit_width=3).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "752dab51-e0c3-484f-8993-50037a67c072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the best model from MLPNBQuantResults_ag_news/ModelParameter_Disc_3bit_0.25_0.25_0.25_0.25.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['fc1.weight_orig', 'fc2.weight_orig'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_model_path = f\"MLPNBQuantResults_ag_news/ModelParameter_Disc_3bit_0.25_0.25_0.25_0.25.pth\"\n",
    "print(f\"Loading the best model from {quant_model_path}\")\n",
    "quant_model.load_state_dict(torch.load(quant_model_path), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "311e2d4d-c511-4c7d-a3d8-ccc98ef773b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 238/238 [00:00<00:00, 249.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.7274, Test Acc: 0.8673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(quant_model,test_iterator,criterion)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81fbac09-682a-41c7-a1b5-15f632b44579",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features, test_labels = extract_features(quant_model, test_iterator, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5800099e-7b85-44da-8fa1-0e7b30ed9fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8605079615738913\n"
     ]
    }
   ],
   "source": [
    "test_features, test_labels = extract_features(quant_model, test_iterator, device)\n",
    "test_preds = nb_model.predict(test_features)\n",
    "final_test_accuracy = accuracy_score(test_labels, test_preds)\n",
    "print(final_test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd4a492-7a1f-4d7a-949e-17f14ffcec94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
