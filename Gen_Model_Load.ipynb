{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c61860c-9413-43c8-b6a7-79471f35231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import spacy\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext import data\n",
    "import pandas as pd\n",
    "from models import Gen\n",
    "import warnings as wrn\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "SEED = 2021\n",
    "\n",
    "wrn.filterwarnings('ignore')\n",
    "os.environ['SP_DIR'] = '/opt/conda/lib/python3.11/site-packages'\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cuda.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8362160f-cfc9-4fd0-b731-551ce101fd8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "389a9f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODE = 'inference' # 'train' or 'inference' or 'none'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489305ff",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc9ceb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/ag_news/train.csv', header=None, usecols=[0,2])\n",
    "train_data.columns = ['label', 'text']\n",
    "valid_data = pd.read_csv('data/ag_news/valid.csv', header=None, usecols=[0,2])\n",
    "valid_data.columns = ['label', 'text']\n",
    "test_data = pd.read_csv('data/ag_news/test.csv', header=None, usecols=[0,2])\n",
    "test_data.columns = ['label', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbb4b372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120000 5000 7600\n"
     ]
    }
   ],
   "source": [
    "TRAIN_SIZE = len(train_data)\n",
    "VALID_SIZE = len(valid_data)\n",
    "TEST_SIZE = len(test_data)\n",
    "print(TRAIN_SIZE, VALID_SIZE, TEST_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b469659",
   "metadata": {},
   "source": [
    "### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c97bebb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_text(text):\n",
    "#     return re.sub(r'[^A-Za-z0-9]+', ' ', str(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1ce9d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data['text'] = train_data['text'].apply(clean_text)\n",
    "# valid_data['text'] = valid_data['text'].apply(clean_text)\n",
    "# test_data['text'] = test_data['text'].apply(clean_text)\n",
    "\n",
    "# train_data.to_csv('data/ag_news3/train_clean.csv', index=False, header=False)\n",
    "# valid_data.to_csv('data/ag_news3/valid_clean.csv', index=False, header=False)\n",
    "# test_data.to_csv('data/ag_news3/test_clean.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84f7acf",
   "metadata": {},
   "source": [
    "### Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fd2877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def spacy_tokenizer(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f4d6f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = data.LabelField()\n",
    "TEXT = data.Field(tokenize=spacy_tokenizer, batch_first=True, include_lengths=True)\n",
    "fields = [('label', LABEL), ('text', TEXT)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "752038e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data.TabularDataset(path='data/ag_news/train_clean.csv', format='csv', fields=fields, skip_header=True)\n",
    "valid_dataset = data.TabularDataset(path='data/ag_news/valid_clean.csv', format='csv', fields=fields, skip_header=True)\n",
    "test_dataset = data.TabularDataset(path='data/ag_news/test_clean.csv', format='csv', fields=fields, skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44c0f16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': '4', 'text': ['Apple', 'yesterday', 'released', 'a', '12', '7', 'MB', 'security', 'update', 'that', 'consists', 'of', 'several', 'revised', 'components', 'including', 'Apache', 'AppKit', 'HIToolbox', 'Kerberos', 'Postfix', 'PSNormalizer', 'Safari', 'and', 'Terminal']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_dataset.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2acb6135",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_dataset, min_freq=5)\n",
    "LABEL.build_vocab(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1b82075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances per class: {'2': 30000, '3': 30000, '4': 30000, '1': 29999}\n",
      "Size of text vocab: 27797\n",
      "Size of label vocab: 4\n"
     ]
    }
   ],
   "source": [
    "label_counts = {LABEL.vocab.itos[i]: LABEL.vocab.freqs[LABEL.vocab.itos[i]] for i in range(len(LABEL.vocab))}\n",
    "print(\"Number of instances per class:\", label_counts)\n",
    "\n",
    "print(\"Size of text vocab:\",len(TEXT.vocab))\n",
    "\n",
    "print(\"Size of label vocab:\",len(LABEL.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d87c885b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 174008),\n",
       " ('to', 96378),\n",
       " ('a', 95595),\n",
       " ('of', 89434),\n",
       " ('in', 76339),\n",
       " ('and', 66138),\n",
       " ('on', 47406),\n",
       " ('s', 43763),\n",
       " ('for', 37311),\n",
       " ('39', 31877)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08fd08da",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
    "    (train_dataset, valid_dataset),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "test_iterator = data.BucketIterator(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b728424",
   "metadata": {},
   "source": [
    "### Define Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4f8a11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE= len(TEXT.vocab)\n",
    "WORD_EMB_DIM = 100\n",
    "LABEL_EMB_DIM = 100\n",
    "HID_DIM = 100\n",
    "NLAYERS = 1\n",
    "NCLASS = len(LABEL.vocab)\n",
    "DROPOUT = 0\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "TIED = False\n",
    "USE_BIAS = False\n",
    "CONCAT_LABEL = 'hidden'\n",
    "AVG_LOSS = False\n",
    "ONE_HOT = False\n",
    "BIT_WIDTH = 8\n",
    "\n",
    "LR = 1e-4\n",
    "LOG_INTERVAL = 200\n",
    "CLIP = 1.0\n",
    "LOGGING = logging.INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "826fcac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Gen(VOCAB_SIZE, WORD_EMB_DIM, LABEL_EMB_DIM, HID_DIM, NLAYERS, NCLASS, DROPOUT, USE_CUDA, TIED, USE_BIAS, CONCAT_LABEL, AVG_LOSS, ONE_HOT).to(device)\n",
    "criterion = nn.CrossEntropyLoss(reduce=False).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ec3ab3c-dd80-457c-818f-3aae25e9cc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_hidden(model, bsz):\n",
    "    weight = next(model.parameters())\n",
    "    # Return hidden state and cell state as 2D tensors\n",
    "    return (weight.new_zeros(NLAYERS, HID_DIM),\n",
    "            weight.new_zeros(NLAYERS, HID_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "537e3186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(valid_iterator, model, criterion, mode='valid', model_state=0):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    cnt = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_iterator, desc=f\"Evaluating ({mode})\", leave=True):\n",
    "            sents = [torch.tensor(row) for row in batch.text[0]]\n",
    "            labels = batch.label\n",
    "            # y_exts = [torch.full((batch.text[0].shape[1],), labels[i], dtype=torch.long) for i in range(len(labels))]\n",
    "            y_exts = []\n",
    "            for y_label in range(NCLASS):\n",
    "                y_ext = []\n",
    "                for d in sents:\n",
    "                    y_ext.append(torch.LongTensor([y_label] * (len(d) - 1)))\n",
    "                y_exts.append(y_ext)\n",
    "            \n",
    "            \n",
    "            hidden = init_hidden(model, len(sents))\n",
    "            x = nn.utils.rnn.pack_sequence([s[:-1] for s in sents])\n",
    "            x_pred = nn.utils.rnn.pack_sequence([s[1:] for s in sents])\n",
    "\n",
    "            # p_y = torch.FloatTensor([0.071] * len(seq_len))\n",
    "\n",
    "            losses = []\n",
    "            for y_ext in y_exts:\n",
    "                y_ext = nn.utils.rnn.pack_sequence(y_ext)\n",
    "\n",
    "                if device.type == 'cuda':\n",
    "                    x, y_ext, x_pred, labels = x.cuda(), y_ext.cuda(), x_pred.cuda(), labels.cuda()\n",
    "\n",
    "                # output (batch_size, )\n",
    "                hidden = init_hidden(model, len(sents))\n",
    "                \n",
    "                out = model(x, x_pred, y_ext, hidden, criterion, model_state)\n",
    "                \n",
    "                loss_matrix = criterion(out, x_pred.data)\n",
    "\n",
    "                LM_loss = nn.utils.rnn.pad_packed_sequence(nn.utils.rnn.PackedSequence(\n",
    "                    loss_matrix, x.batch_sizes))[0].transpose(0,1)\n",
    "                sum_loss = torch.sum(LM_loss, dim = 1)\n",
    "                \n",
    "                losses.append(sum_loss)\n",
    "\n",
    "            losses = torch.cat(losses, dim=0).view(-1, len(sents))\n",
    "            prediction = torch.argmin(losses, dim=0)\n",
    "\n",
    "            num_correct = (prediction == labels).float().sum()\n",
    "\n",
    "            total_loss += torch.sum(torch.min(losses, dim=0)[0]).item()\n",
    "            total_correct += num_correct.item()\n",
    "            cnt += 1\n",
    "\n",
    "    return total_loss / cnt, total_correct / (VALID_SIZE if mode == 'valid' else TEST_SIZE if mode == 'test' else '0') * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e011f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating (test): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 238/238 [00:04<00:00, 57.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "Test Loss: 7226.538382393973 | Test Acc:  88.57894736842105\n",
      "=========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# if MODE == 'inference':\n",
    "#     model.load_state_dict(torch.load('gen_lstm_spacy_best_val.pth'))\n",
    "#     model.eval()\n",
    "    \n",
    "#     test_loss, test_acc = evaluate(test_iterator, model, criterion, mode='test', model_state='fp')\n",
    "#     print('=' * 89)\n",
    "#     print(f'Test Loss: {test_loss} | Test Acc:  {test_acc}')\n",
    "#     print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83083b8c-149b-45be-9859-abf50926c82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brevitas.graph.quantize import preprocess_for_quantize\n",
    "from ptq_common import quantize_model, apply_bias_correction, apply_act_equalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85d518c5-d36d-4689-bb4b-67b524b45527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_model = preprocess_for_quantize(\n",
    "#             model,\n",
    "#             equalize_iters=20,\n",
    "#             equalize_merge_bias=True,\n",
    "#             merge_bn=True,\n",
    "#             channel_splitting_ratio=0.0,\n",
    "#             channel_splitting_split_input=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a677b6c6-749b-441a-b026-1827f70e4222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is set to  cpu\n",
      "Quantizing the model\n",
      "Quantization completed!\n",
      "device is set back to  cuda\n"
     ]
    }
   ],
   "source": [
    "dtype = getattr(torch, 'float')\n",
    "device = torch.device('cpu')\n",
    "print(\"device is set to \",device)\n",
    "print(\"Quantizing the model\")\n",
    "quant_model = quantize_model(\n",
    "        model.to(device),\n",
    "        dtype=dtype,\n",
    "        device=device,\n",
    "        backend='layerwise',\n",
    "        scale_factor_type='float_scale',\n",
    "        bias_bit_width=32,\n",
    "        weight_bit_width=BIT_WIDTH,\n",
    "        weight_narrow_range=False,\n",
    "        weight_param_method='stats',\n",
    "        weight_quant_granularity='per_tensor',\n",
    "        weight_quant_type='sym',\n",
    "        layerwise_first_last_bit_width=BIT_WIDTH,\n",
    "        act_bit_width=BIT_WIDTH,\n",
    "        act_param_method='stats',\n",
    "        act_quant_percentile=99.99,\n",
    "        act_quant_type='sym',\n",
    "        quant_format='int',\n",
    "        layerwise_first_last_mantissa_bit_width=4,\n",
    "        layerwise_first_last_exponent_bit_width=3,\n",
    "        weight_mantissa_bit_width=4,\n",
    "        weight_exponent_bit_width=3,\n",
    "        act_mantissa_bit_width=4,\n",
    "        act_exponent_bit_width=3).to(device) \n",
    "\n",
    "print(\"Quantization completed!\")\n",
    "device = torch.device('cuda')\n",
    "print(\"device is set back to \",device)\n",
    "model=model.to(device)\n",
    "quant_model=quant_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "379defec-8768-477a-a15f-4c02ee0c2c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_model.load_state_dict(torch.load(f'./GenQuantResults/ModelParameter_Gen_{BIT_WIDTH}bit.pth'))\n",
    "quant_model=quant_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be42581c-1ab6-41b2-9207-ad587be8a3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating (test):   0%|                                                                                                                                                                | 0/238 [00:14<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_loss, test_acc \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquant\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m89\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Test Acc before calibration:  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 36\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(valid_iterator, model, criterion, mode, model_state)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# output (batch_size, )\u001b[39;00m\n\u001b[1;32m     34\u001b[0m hidden \u001b[38;5;241m=\u001b[39m init_hidden(model, \u001b[38;5;28mlen\u001b[39m(sents))\n\u001b[0;32m---> 36\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_ext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m loss_matrix \u001b[38;5;241m=\u001b[39m criterion(out, x_pred\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m     40\u001b[0m LM_loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mrnn\u001b[38;5;241m.\u001b[39mpad_packed_sequence(nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mrnn\u001b[38;5;241m.\u001b[39mPackedSequence(\n\u001b[1;32m     41\u001b[0m     loss_matrix, x\u001b[38;5;241m.\u001b[39mbatch_sizes))[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/video-proj-storage/mushfiq_files/PTQ_Noise/models.py:74\u001b[0m, in \u001b[0;36mGen.forward\u001b[0;34m(self, x, x_pred, y_ext, hidden, criterion, model_state)\u001b[0m\n\u001b[1;32m     71\u001b[0m embedded_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_encoder(y_ext\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m#print(\"embedded_label.shape:\", embedded_label.shape)\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m output, (_, _) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded_sents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m#print(\"output.shape:\", output.shape)\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_state \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquant\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;66;03m#calibration of quantized model\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/brevitas/nn/quant_rnn.py:984\u001b[0m, in \u001b[0;36mQuantLSTM.forward\u001b[0;34m(self, inp, hx, cx)\u001b[0m\n\u001b[1;32m    982\u001b[0m layer_hidden_state \u001b[38;5;241m=\u001b[39m hx[\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m l \u001b[38;5;241m+\u001b[39m d] \u001b[38;5;28;01mif\u001b[39;00m hx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m hx\n\u001b[1;32m    983\u001b[0m layer_cell_state \u001b[38;5;241m=\u001b[39m cx[\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m l \u001b[38;5;241m+\u001b[39m d] \u001b[38;5;28;01mif\u001b[39;00m cx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m cx\n\u001b[0;32m--> 984\u001b[0m out, out_hidden_state, out_cell_state \u001b[38;5;241m=\u001b[39m \u001b[43mdirection\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_hidden_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_cell_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    985\u001b[0m dir_outputs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [out]\n\u001b[1;32m    986\u001b[0m dir_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [out_hidden_state]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/brevitas/nn/quant_rnn.py:713\u001b[0m, in \u001b[0;36m_QuantLSTMLayer.forward\u001b[0;34m(self, inp, hidden_state, cell_state)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    712\u001b[0m     cell \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell\n\u001b[0;32m--> 713\u001b[0m quant_outputs, quant_hidden_state, quant_cell_state \u001b[38;5;241m=\u001b[39m \u001b[43mcell\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_input_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_unpack_quant_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_hidden_state\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_unpack_quant_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_cell_state\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_weight_ii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_unpack_quant_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_weight_ii\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_weight_if\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_unpack_quant_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_weight_if\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_weight_ic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_unpack_quant_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_weight_ic\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_weight_io\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_unpack_quant_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_weight_io\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_weight_hi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_unpack_quant_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_weight_hi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_weight_hf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_unpack_quant_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_weight_hf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_weight_hc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_unpack_quant_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_weight_hc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_weight_ho\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_unpack_quant_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_weight_ho\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_bias_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_bias_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_bias_forget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_bias_forget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_bias_cell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_bias_cell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_bias_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_bias_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    729\u001b[0m quant_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpack_quant_outputs(quant_outputs)\n\u001b[1;32m    730\u001b[0m quant_hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpack_quant_state(quant_hidden_state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell\u001b[38;5;241m.\u001b[39moutput_quant)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/brevitas/nn/quant_rnn.py:300\u001b[0m, in \u001b[0;36m_QuantLSTMCell.forward\u001b[0;34m(self, quant_input, quant_hidden_state, quant_cell_state, quant_weight_ii, quant_weight_if, quant_weight_ic, quant_weight_io, quant_weight_hi, quant_weight_hf, quant_weight_hc, quant_weight_ho, quant_bias_input, quant_bias_forget, quant_bias_cell, quant_bias_output)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(end):\n\u001b[1;32m    299\u001b[0m     quant_input \u001b[38;5;241m=\u001b[39m quant_inputs[index]\n\u001b[0;32m--> 300\u001b[0m     quant_hidden_state_tuple, quant_cell_state_tuple \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_iter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_hidden_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_cell_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_weight_ii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_weight_if\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_weight_ic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_weight_io\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_weight_hi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_weight_hf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_weight_hc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_weight_ho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_bias_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_bias_forget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_bias_cell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_bias_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m     index \u001b[38;5;241m=\u001b[39m index \u001b[38;5;241m+\u001b[39m step\n\u001b[1;32m    317\u001b[0m     quant_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [quant_hidden_state_tuple]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/brevitas/nn/quant_rnn.py:262\u001b[0m, in \u001b[0;36m_QuantLSTMCell.forward_iter\u001b[0;34m(self, quant_input, quant_hidden_state, quant_cell_state, quant_weight_ii, quant_weight_if, quant_weight_ic, quant_weight_io, quant_weight_hi, quant_weight_hf, quant_weight_hc, quant_weight_ho, quant_bias_input, quant_bias_forget, quant_bias_cell, quant_bias_output)\u001b[0m\n\u001b[1;32m    260\u001b[0m quant_forget_cell \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell_state_quant(quant_forget_gate \u001b[38;5;241m*\u001b[39m quant_cell_state)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    261\u001b[0m quant_inp_cell \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell_state_quant(quant_input_gate \u001b[38;5;241m*\u001b[39m quant_cell_gate)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 262\u001b[0m quant_cell_state_tuple \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcell_state_quant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_forget_cell\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mquant_inp_cell\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m quant_hidden_state_tanh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_state_tanh_quant(quant_cell_state_tuple[\u001b[38;5;241m0\u001b[39m])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    264\u001b[0m quant_hidden_state \u001b[38;5;241m=\u001b[39m quant_out_gate \u001b[38;5;241m*\u001b[39m quant_hidden_state_tanh\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/brevitas/proxy/runtime_quant.py:88\u001b[0m, in \u001b[0;36mFusedActivationQuantProxy.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;129m@brevitas\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mscript_method\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     87\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_impl(x)\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor_quant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/brevitas/core/quant/int.py:159\u001b[0m, in \u001b[0;36mRescalingIntQuant.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    157\u001b[0m     y \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint_quant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_point\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbit_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y, scale, zero_point, bit_width\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/brevitas/core/quant/int_base.py:87\u001b[0m, in \u001b[0;36mIntQuant.forward\u001b[0;34m(self, scale, zero_point, bit_width, x)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;129m@brevitas\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mscript_method\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, scale: Tensor, zero_point: Tensor, bit_width: Tensor, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 87\u001b[0m     y_int \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_point\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbit_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     y \u001b[38;5;241m=\u001b[39m y_int \u001b[38;5;241m-\u001b[39m zero_point\n\u001b[1;32m     89\u001b[0m     y \u001b[38;5;241m=\u001b[39m y \u001b[38;5;241m*\u001b[39m scale\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/brevitas/core/quant/int_base.py:74\u001b[0m, in \u001b[0;36mIntQuant.to_int\u001b[0;34m(self, scale, zero_point, bit_width, x)\u001b[0m\n\u001b[1;32m     72\u001b[0m max_int_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_int(bit_width)\n\u001b[1;32m     73\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfloat_to_int_impl(y)\n\u001b[0;32m---> 74\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor_clamp_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_int_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_int_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(test_iterator, quant_model, criterion, mode='test', model_state='quant')\n",
    "print('=' * 89)\n",
    "print(f'Test Loss: {test_loss} | Test Acc before calibration:  {test_acc}')\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eddc7e9-e06e-481e-a486-663f162339db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
